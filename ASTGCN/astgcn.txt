root@autodl-container-8fd941ac7c-1b48858e:~/autodl-tmp/Traffic-Flow-Forecasting/ASTGCN# python train_ASTGCN_r.py 
Read configuration file: ./configurations/PEMS04_astgcn.conf
CUDA: True cuda:0
folder_dir: astgcn_r_h1d0w0_channel1_1.000000e-03
params_path: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03
load file: ./data/PEMS04/PEMS04_r1_d0_w0_astcgn
train: torch.Size([10181, 307, 1, 12]) torch.Size([10181, 307, 12])
val: torch.Size([3394, 307, 1, 12]) torch.Size([3394, 307, 12])
test: torch.Size([3394, 307, 1, 12]) torch.Size([3394, 307, 12])
create params directory experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03
param list:
CUDA     cuda:0
in_channels      1
nb_block         2
nb_chev_filter   64
nb_time_filter   64
time_strides     1
batch_size       32
graph_signal_matrix_filename     ./data/PEMS04/PEMS04.npz
start_epoch      0
epochs   80
ASTGCN_submodule(
  (BlockList): ModuleList(
    (0): ASTGCN_block(
      (TAt): Temporal_Attention_layer()
      (SAt): Spatial_Attention_layer()
      (cheb_conv_SAt): cheb_conv_withSAt(
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
        )
      )
      (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (residual_conv): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))
      (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (1): ASTGCN_block(
      (TAt): Temporal_Attention_layer()
      (SAt): Spatial_Attention_layer()
      (cheb_conv_SAt): cheb_conv_withSAt(
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
        )
      )
      (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (residual_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (final_conv): Conv2d(12, 12, kernel_size=(1, 64), stride=(1, 1))
)
Net's state_dict:
BlockList.0.TAt.U1       torch.Size([307])
BlockList.0.TAt.U2       torch.Size([1, 307])
BlockList.0.TAt.U3       torch.Size([1])
BlockList.0.TAt.be       torch.Size([1, 12, 12])
BlockList.0.TAt.Ve       torch.Size([12, 12])
BlockList.0.SAt.W1       torch.Size([12])
BlockList.0.SAt.W2       torch.Size([1, 12])
BlockList.0.SAt.W3       torch.Size([1])
BlockList.0.SAt.bs       torch.Size([1, 307, 307])
BlockList.0.SAt.Vs       torch.Size([307, 307])
BlockList.0.cheb_conv_SAt.Theta.0        torch.Size([1, 64])
BlockList.0.cheb_conv_SAt.Theta.1        torch.Size([1, 64])
BlockList.0.cheb_conv_SAt.Theta.2        torch.Size([1, 64])
BlockList.0.time_conv.weight     torch.Size([64, 64, 1, 3])
BlockList.0.time_conv.bias       torch.Size([64])
BlockList.0.residual_conv.weight         torch.Size([64, 1, 1, 1])
BlockList.0.residual_conv.bias   torch.Size([64])
BlockList.0.ln.weight    torch.Size([64])
BlockList.0.ln.bias      torch.Size([64])
BlockList.1.TAt.U1       torch.Size([307])
BlockList.1.TAt.U2       torch.Size([64, 307])
BlockList.1.TAt.U3       torch.Size([64])
BlockList.1.TAt.be       torch.Size([1, 12, 12])
BlockList.1.TAt.Ve       torch.Size([12, 12])
BlockList.1.SAt.W1       torch.Size([12])
BlockList.1.SAt.W2       torch.Size([64, 12])
BlockList.1.SAt.W3       torch.Size([64])
BlockList.1.SAt.bs       torch.Size([1, 307, 307])
BlockList.1.SAt.Vs       torch.Size([307, 307])
BlockList.1.cheb_conv_SAt.Theta.0        torch.Size([64, 64])
BlockList.1.cheb_conv_SAt.Theta.1        torch.Size([64, 64])
BlockList.1.cheb_conv_SAt.Theta.2        torch.Size([64, 64])
BlockList.1.time_conv.weight     torch.Size([64, 64, 1, 3])
BlockList.1.time_conv.bias       torch.Size([64])
BlockList.1.residual_conv.weight         torch.Size([64, 64, 1, 1])
BlockList.1.residual_conv.bias   torch.Size([64])
BlockList.1.ln.weight    torch.Size([64])
BlockList.1.ln.bias      torch.Size([64])
final_conv.weight        torch.Size([12, 12, 1, 64])
final_conv.bias          torch.Size([12])
Net's total params: 450031
Optimizer's state_dict:
state    {}
param_groups     [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]}]
validation batch 1 / 107, loss: 275.19
validation batch 101 / 107, loss: 331.35
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_0.params
validation batch 1 / 107, loss: 56.77
validation batch 101 / 107, loss: 85.06
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_1.params
validation batch 1 / 107, loss: 35.95
validation batch 101 / 107, loss: 41.87
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_2.params
validation batch 1 / 107, loss: 33.44
validation batch 101 / 107, loss: 36.77
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_3.params
global step: 1000, training loss: 30.47, time: 60.48s
validation batch 1 / 107, loss: 32.01
validation batch 101 / 107, loss: 34.45
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_4.params
validation batch 1 / 107, loss: 31.68
validation batch 101 / 107, loss: 34.14
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_5.params
validation batch 1 / 107, loss: 30.27
validation batch 101 / 107, loss: 33.44
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_6.params
global step: 2000, training loss: 27.26, time: 119.56s
validation batch 1 / 107, loss: 29.26
validation batch 101 / 107, loss: 32.82
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_7.params
validation batch 1 / 107, loss: 31.89
validation batch 101 / 107, loss: 34.72
validation batch 1 / 107, loss: 28.50
validation batch 101 / 107, loss: 32.08
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_9.params
global step: 3000, training loss: 25.09, time: 178.51s
validation batch 1 / 107, loss: 28.58
validation batch 101 / 107, loss: 31.77
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_10.params
validation batch 1 / 107, loss: 28.51
validation batch 101 / 107, loss: 31.54
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_11.params
validation batch 1 / 107, loss: 28.11
validation batch 101 / 107, loss: 30.94
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_12.params
global step: 4000, training loss: 23.23, time: 237.45s
validation batch 1 / 107, loss: 27.32
validation batch 101 / 107, loss: 30.72
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_13.params
validation batch 1 / 107, loss: 27.40
validation batch 101 / 107, loss: 30.23
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_14.params
validation batch 1 / 107, loss: 27.06
validation batch 101 / 107, loss: 30.22
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_15.params
global step: 5000, training loss: 23.89, time: 296.60s
validation batch 1 / 107, loss: 27.28
validation batch 101 / 107, loss: 30.07
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_16.params
validation batch 1 / 107, loss: 27.82
validation batch 101 / 107, loss: 30.25
validation batch 1 / 107, loss: 27.35
validation batch 101 / 107, loss: 29.68
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_18.params
global step: 6000, training loss: 20.10, time: 355.71s
validation batch 1 / 107, loss: 26.83
validation batch 101 / 107, loss: 29.83
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_19.params
validation batch 1 / 107, loss: 27.30
validation batch 101 / 107, loss: 30.32
validation batch 1 / 107, loss: 27.25
validation batch 101 / 107, loss: 30.33
global step: 7000, training loss: 21.23, time: 415.11s
validation batch 1 / 107, loss: 26.99
validation batch 101 / 107, loss: 30.02
validation batch 1 / 107, loss: 28.87
validation batch 101 / 107, loss: 31.63
validation batch 1 / 107, loss: 26.79
validation batch 101 / 107, loss: 29.62
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_24.params
validation batch 1 / 107, loss: 26.57
validation batch 101 / 107, loss: 29.46
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_25.params
global step: 8000, training loss: 21.81, time: 475.99s
validation batch 1 / 107, loss: 28.23
validation batch 101 / 107, loss: 30.70
validation batch 1 / 107, loss: 26.25
validation batch 101 / 107, loss: 29.55
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_27.params
validation batch 1 / 107, loss: 26.95
validation batch 101 / 107, loss: 30.23
global step: 9000, training loss: 22.60, time: 535.08s
validation batch 1 / 107, loss: 26.79
validation batch 101 / 107, loss: 30.14
validation batch 1 / 107, loss: 26.24
validation batch 101 / 107, loss: 29.21
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_30.params
validation batch 1 / 107, loss: 26.27
validation batch 101 / 107, loss: 29.47
global step: 10000, training loss: 21.11, time: 593.81s
validation batch 1 / 107, loss: 26.36
validation batch 101 / 107, loss: 29.86
validation batch 1 / 107, loss: 26.55
validation batch 101 / 107, loss: 29.94
validation batch 1 / 107, loss: 26.08
validation batch 101 / 107, loss: 29.57
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_34.params
global step: 11000, training loss: 20.66, time: 652.52s
validation batch 1 / 107, loss: 27.17
validation batch 101 / 107, loss: 30.45
validation batch 1 / 107, loss: 26.25
validation batch 101 / 107, loss: 29.53
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_36.params
validation batch 1 / 107, loss: 26.05
validation batch 101 / 107, loss: 29.52
global step: 12000, training loss: 19.10, time: 711.57s
validation batch 1 / 107, loss: 27.06
validation batch 101 / 107, loss: 29.65
validation batch 1 / 107, loss: 26.11
validation batch 101 / 107, loss: 29.32
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_39.params
validation batch 1 / 107, loss: 26.12
validation batch 101 / 107, loss: 29.59
global step: 13000, training loss: 16.22, time: 770.66s
validation batch 1 / 107, loss: 26.44
validation batch 101 / 107, loss: 29.41
validation batch 1 / 107, loss: 26.02
validation batch 101 / 107, loss: 29.52
validation batch 1 / 107, loss: 26.16
validation batch 101 / 107, loss: 29.17
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_43.params
global step: 14000, training loss: 23.09, time: 829.45s
validation batch 1 / 107, loss: 26.18
validation batch 101 / 107, loss: 29.88
validation batch 1 / 107, loss: 26.01
validation batch 101 / 107, loss: 29.43
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_45.params
validation batch 1 / 107, loss: 26.09
validation batch 101 / 107, loss: 29.90
validation batch 1 / 107, loss: 28.18
validation batch 101 / 107, loss: 31.00
global step: 15000, training loss: 20.16, time: 890.52s
validation batch 1 / 107, loss: 26.03
validation batch 101 / 107, loss: 29.31
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_48.params
validation batch 1 / 107, loss: 26.07
validation batch 101 / 107, loss: 29.33
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_49.params
validation batch 1 / 107, loss: 26.89
validation batch 101 / 107, loss: 30.52
global step: 16000, training loss: 22.14, time: 949.74s
validation batch 1 / 107, loss: 26.08
validation batch 101 / 107, loss: 30.14
validation batch 1 / 107, loss: 26.22
validation batch 101 / 107, loss: 29.88
validation batch 1 / 107, loss: 25.91
validation batch 101 / 107, loss: 29.36
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_53.params
global step: 17000, training loss: 20.25, time: 1008.59s
validation batch 1 / 107, loss: 26.06
validation batch 101 / 107, loss: 29.65
validation batch 1 / 107, loss: 26.31
validation batch 101 / 107, loss: 30.20
validation batch 1 / 107, loss: 26.06
validation batch 101 / 107, loss: 29.74
global step: 18000, training loss: 19.82, time: 1067.61s
validation batch 1 / 107, loss: 26.00
validation batch 101 / 107, loss: 29.86
validation batch 1 / 107, loss: 26.03
validation batch 101 / 107, loss: 30.07
validation batch 1 / 107, loss: 25.98
validation batch 101 / 107, loss: 30.13
global step: 19000, training loss: 19.90, time: 1126.52s
validation batch 1 / 107, loss: 26.41
validation batch 101 / 107, loss: 30.32
validation batch 1 / 107, loss: 26.32
validation batch 101 / 107, loss: 30.13
validation batch 1 / 107, loss: 26.20
validation batch 101 / 107, loss: 29.89
global step: 20000, training loss: 17.73, time: 1185.38s
validation batch 1 / 107, loss: 25.86
validation batch 101 / 107, loss: 29.50
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_63.params
validation batch 1 / 107, loss: 26.18
validation batch 101 / 107, loss: 29.79
validation batch 1 / 107, loss: 25.92
validation batch 101 / 107, loss: 29.52
global step: 21000, training loss: 18.36, time: 1244.48s
validation batch 1 / 107, loss: 25.83
validation batch 101 / 107, loss: 29.58
validation batch 1 / 107, loss: 26.51
validation batch 101 / 107, loss: 30.41
validation batch 1 / 107, loss: 26.06
validation batch 101 / 107, loss: 29.62
global step: 22000, training loss: 18.80, time: 1303.50s
validation batch 1 / 107, loss: 27.22
validation batch 101 / 107, loss: 31.16
validation batch 1 / 107, loss: 25.97
validation batch 101 / 107, loss: 29.75
validation batch 1 / 107, loss: 26.13
validation batch 101 / 107, loss: 30.27
validation batch 1 / 107, loss: 25.98
validation batch 101 / 107, loss: 29.80
global step: 23000, training loss: 19.71, time: 1364.37s
validation batch 1 / 107, loss: 26.25
validation batch 101 / 107, loss: 30.44
validation batch 1 / 107, loss: 25.83
validation batch 101 / 107, loss: 29.63
validation batch 1 / 107, loss: 25.97
validation batch 101 / 107, loss: 29.55
global step: 24000, training loss: 20.64, time: 1423.25s
validation batch 1 / 107, loss: 25.81
validation batch 101 / 107, loss: 30.00
validation batch 1 / 107, loss: 25.78
validation batch 101 / 107, loss: 29.96
validation batch 1 / 107, loss: 26.04
validation batch 101 / 107, loss: 30.11
global step: 25000, training loss: 17.29, time: 1482.35s
validation batch 1 / 107, loss: 25.81
validation batch 101 / 107, loss: 29.92
best epoch: 63
load weight from: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_63.params
predicting data set batch 1 / 107
predicting data set batch 101 / 107
input: (3394, 307, 1, 12)
prediction: (3394, 307, 12)
data_target_tensor: (3394, 307, 12)
current epoch: 63, predict 0 points
MAE: 17.73
RMSE: 28.26
MAPE: 0.12
current epoch: 63, predict 1 points
MAE: 18.84
RMSE: 29.93
MAPE: 0.13
current epoch: 63, predict 2 points
MAE: 19.70
RMSE: 31.20
MAPE: 0.13
current epoch: 63, predict 3 points
MAE: 20.40
RMSE: 32.23
MAPE: 0.14
current epoch: 63, predict 4 points
MAE: 20.99
RMSE: 33.13
MAPE: 0.14
current epoch: 63, predict 5 points
MAE: 21.60
RMSE: 33.99
MAPE: 0.14
current epoch: 63, predict 6 points
MAE: 22.24
RMSE: 34.92
MAPE: 0.15
current epoch: 63, predict 7 points
MAE: 22.83
RMSE: 35.81
MAPE: 0.15
current epoch: 63, predict 8 points
MAE: 23.44
RMSE: 36.68
MAPE: 0.15
current epoch: 63, predict 9 points
MAE: 24.05
RMSE: 37.54
MAPE: 0.16
current epoch: 63, predict 10 points
MAE: 24.84
RMSE: 38.63
MAPE: 0.16
current epoch: 63, predict 11 points
MAE: 25.77
RMSE: 39.90
MAPE: 0.17
all MAE: 21.87
all RMSE: 34.52
all MAPE: 0.14
[17.727335, 28.26373875167007, 0.11813516, 18.835678, 29.927113811947876, 0.12532474, 19.701342, 31.204095190225033, 0.13094154, 20.399353, 32.22846301214192, 0.13538572, 20.994793, 33.127367618806694, 0.139136, 21.604166, 33.98521191498223, 0.14271508, 22.236315, 34.91797377537999, 0.14684333, 22.833164, 35.80655832582384, 0.15080237, 23.43834, 36.682881417120164, 0.15465686, 24.047075, 37.542680789461144, 0.15888481, 24.843225, 38.62646323583581, 0.16400942, 25.76663, 39.899548857392325, 0.17003986, 21.868965, 34.520162247047736, 0.14473987]