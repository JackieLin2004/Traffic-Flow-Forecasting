import os
import re
import time
import h5py
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

from traffic_dataset import LoadData
from utils import Evaluation  # ä¸‰ç§è¯„ä»·æŒ‡æ ‡ä»¥åŠå¯è§†åŒ–ç±»
from utils import visualize_result
from gcnnet import GCN
from chebnet import ChebNet
from gat import GATNet
from rich import print
from tqdm import tqdm

import warnings

warnings.filterwarnings('ignore')


def main():
    # é…ç½®æ—¥å¿—æ–‡ä»¶
    log_file = "GAT_08_training_log.txt" ##todo
    if os.path.exists(log_file):
        os.remove(log_file)

    os.environ["CUDA_VISIBLE_DEVICES"] = "0"

    # ç¬¬ä¸€æ­¥ï¼štodo å‡†å¤‡æ•°æ®
    train_data = LoadData(data_path=["PeMS_08/PeMS08.csv", "PeMS_08/PeMS08.npz"], num_nodes=170, divide_days=[45, 14],
                          time_interval=5, history_length=6,
                          train_mode="train")

    # num_workersæ˜¯åŠ è½½æ•°æ®ï¼ˆbatchï¼‰çš„çº¿ç¨‹æ•°ç›®
    train_loader = DataLoader(
        train_data, batch_size=32, shuffle=True, num_workers=4)
    # todo
    test_data = LoadData(data_path=["PeMS_08/PeMS08.csv", "PeMS_08/PeMS08.npz"], num_nodes=170, divide_days=[45, 14],
                         time_interval=5, history_length=6,
                         train_mode="test")

    test_loader = DataLoader(test_data, batch_size=32,
                             shuffle=False, num_workers=4)
    print("ğŸš€ğŸš€ğŸš€ [italic bold green]æ•°æ®åŠ è½½å®Œæˆ!!!")

    # todo SECTION: ç¬¬äºŒæ­¥ï¼šå®šä¹‰æ¨¡å‹ï¼ˆè¿™é‡Œå…¶å®åªæ˜¯åŠ è½½æ¨¡å‹ï¼Œå…³äºæ¨¡å‹çš„å®šä¹‰åœ¨ä¸‹é¢å•ç‹¬å†™äº†ï¼Œå…ˆå‡è®¾å·²ç»å†™å¥½ï¼‰
    # my_net = GCN(in_c=6, hid_c=6, out_c=1)  # åŠ è½½GCNæ¨¡å‹
    # my_net = ChebNet(in_c=6, hid_c=6, out_c=1, K=2)   # åŠ è½½ChebNetæ¨¡å‹
    my_net = GATNet(in_c=6 * 1, hid_c=6, out_c=1, n_heads=2)  # åŠ è½½GATæ¨¡å‹
    print(my_net)

    device = torch.device(
        "cuda" if torch.cuda.is_available() else "cpu")  # å®šä¹‰è®¾å¤‡

    my_net = my_net.to(device)  # æ¨¡å‹é€å…¥è®¾å¤‡

    # ç¬¬ä¸‰æ­¥ï¼šå®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.MSELoss()  # å‡æ–¹æŸå¤±å‡½æ•°

    # æ²¡å†™å­¦ä¹ ç‡ï¼Œè¡¨ç¤ºä½¿ç”¨çš„æ˜¯é»˜è®¤çš„ï¼Œä¹Ÿå°±æ˜¯lr=1e-3
    optimizer = optim.Adam(params=my_net.parameters())

    # ç¬¬å››æ­¥ï¼šè®­ç»ƒ+æµ‹è¯•
    # Train model
    Epoch = 80  # todo è®­ç»ƒçš„æ¬¡æ•°

    my_net.train()  # æ‰“å¼€è®­ç»ƒæ¨¡å¼
    with open(log_file, "a") as log:
        for epoch in tqdm(range(Epoch), colour="green", desc="Train"):
            epoch_loss = 0.0
            count = 0
            start_time = time.time()
            for data in train_loader:
                my_net.zero_grad()  # æ¢¯åº¦æ¸…é›¶
                count += 1
                # [B, N, 1, D],ç”±äºæ ‡ç­¾flow_yåœ¨cpuä¸­ï¼Œæ‰€ä»¥æœ€åçš„é¢„æµ‹å€¼è¦æ”¾å›åˆ°cpuä¸­
                predict_value = my_net(data, device).to(torch.device("cpu"))

                # è®¡ç®—æŸå¤±ï¼Œåˆ‡è®°è¿™ä¸ªlossä¸æ˜¯æ ‡é‡
                loss = criterion(predict_value, data["flow_y"])

                epoch_loss += loss.item()  # æŠŠä¸€ä¸ªepochçš„æŸå¤±éƒ½åŠ èµ·æ¥ï¼Œæœ€åå†é™¤è®­ç»ƒæ•°æ®é•¿åº¦ï¼Œç”¨å¹³å‡lossæ¥è¡¨ç¤º

                loss.backward()  # åå‘ä¼ æ’­

                optimizer.step()  # æ›´æ–°å‚æ•°
            end_time = time.time()
            # å°†è®­ç»ƒæŸå¤±å†™å…¥æ—¥å¿—æ–‡ä»¶
            log_entry = "Epoch: {:04d}, Loss: {:02.4f}, Time: {:02.2f} mins\n".format(
                epoch, 1000 * epoch_loss / len(train_data), (end_time - start_time) / 60
            )
            log.write(log_entry)
            print(log_entry.strip())  # æ‰“å°åˆ°æ§åˆ¶å°


    # Test Model
    # å¯¹äºæµ‹è¯•:
    # ç¬¬ä¸€ã€é™¤äº†è®¡ç®—lossä¹‹å¤–ï¼Œè¿˜éœ€è¦å¯è§†åŒ–ä¸€ä¸‹é¢„æµ‹çš„ç»“æœï¼ˆå®šæ€§åˆ†æï¼‰
    # ç¬¬äºŒã€å¯¹äºé¢„æµ‹çš„ç»“æœè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº† MAE, MAPE, and RMSE è¿™ä¸‰ç§è¯„ä»·æ ‡å‡†æ¥è¯„ä¼°ï¼ˆå®šé‡åˆ†æï¼‰
    my_net.eval()  # æ‰“å¼€æµ‹è¯•æ¨¡å¼
    with torch.no_grad():  # å…³é—­æ¢¯åº¦
        MAE, MAPE, RMSE = [], [], []  # å®šä¹‰ä¸‰ç§æŒ‡æ ‡çš„åˆ—è¡¨
        Target = np.zeros([170, 1, 1])  # [N, T, D],T=1 ï¼ƒ ç›®æ ‡æ•°æ®çš„ç»´åº¦ï¼Œç”¨ï¼å¡«å……
        Predict = np.zeros_like(Target)  # [N, T, D],T=1 # é¢„æµ‹æ•°æ®çš„ç»´åº¦

        total_loss = 0.0
        with open(log_file, "a") as log:
            for data in test_loader:  # ä¸€æ¬¡æŠŠä¸€ä¸ªbatchçš„æµ‹è¯•æ•°æ®å–å‡ºæ¥

                # ä¸‹é¢å¾—åˆ°çš„é¢„æµ‹ç»“æœå®é™…ä¸Šæ˜¯å½’ä¸€åŒ–çš„ç»“æœï¼Œæœ‰ä¸€ä¸ªé—®é¢˜æ˜¯æˆ‘ä»¬è¿™é‡Œä½¿ç”¨çš„ä¸‰ç§è¯„ä»·æ ‡å‡†ä»¥åŠå¯è§†åŒ–ç»“æœè¦ç”¨çš„æ˜¯é€†å½’ä¸€åŒ–çš„æ•°æ®
                # [B, N, 1, D]ï¼ŒBæ˜¯batch_size, Næ˜¯èŠ‚ç‚¹æ•°é‡,1æ˜¯æ—¶é—´T=1, Dæ˜¯èŠ‚ç‚¹çš„æµé‡ç‰¹å¾
                predict_value = my_net(data, device).to(torch.device("cpu"))

                loss = criterion(predict_value, data["flow_y"])  # ä½¿ç”¨MSEè®¡ç®—loss

                total_loss += loss.item()  # æ‰€æœ‰çš„batchçš„lossç´¯åŠ 
                # ä¸‹é¢å®é™…ä¸Šæ˜¯æŠŠé¢„æµ‹å€¼å’Œç›®æ ‡å€¼çš„batchæ”¾åˆ°ç¬¬äºŒç»´çš„æ—¶é—´ç»´åº¦ï¼Œè¿™æ˜¯å› ä¸ºåœ¨æµ‹è¯•æ•°æ®çš„æ—¶å€™å¯¹æ ·æœ¬æ²¡æœ‰shuffleï¼Œ
                # æ‰€ä»¥æ¯ä¸€ä¸ªbatchå–å‡ºæ¥çš„æ•°æ®å°±æ˜¯æŒ‰æ—¶é—´é¡ºåºæ¥çš„ï¼Œå› æ­¤æ”¾åˆ°ç¬¬äºŒç»´æ¥è¡¨ç¤ºæ—¶é—´æ˜¯åˆç†çš„.
                predict_value = predict_value.transpose(0, 2).squeeze(
                    0)  # [1, N, B(T), D] -> [N, B(T), D] -> [N, T, D]
                target_value = data["flow_y"].transpose(0, 2).squeeze(
                    0)  # [1, N, B(T), D] -> [N, B(T), D] -> [N, T, D]

                performance, data_to_save = compute_performance(
                    predict_value, target_value, test_loader)  # è®¡ç®—æ¨¡å‹çš„æ€§èƒ½ï¼Œè¿”å›è¯„ä»·ç»“æœå’Œæ¢å¤å¥½çš„æ•°æ®

                # ä¸‹é¢è¿™ä¸ªæ˜¯æ¯ä¸€ä¸ªbatchå–å‡ºçš„æ•°æ®ï¼ŒæŒ‰batchè¿™ä¸ªç»´åº¦è¿›è¡Œä¸²è”ï¼Œæœ€åå°±å¾—åˆ°äº†æ•´ä¸ªæ—¶é—´çš„æ•°æ®ï¼Œä¹Ÿå°±æ˜¯
                # [N, T, D] = [N, T1+T2+..., D]
                Predict = np.concatenate([Predict, data_to_save[0]], axis=1)
                Target = np.concatenate([Target, data_to_save[1]], axis=1)

                MAE.append(performance[0])
                MAPE.append(performance[1])
                RMSE.append(performance[2])

                log_entry = "Test Loss: {:02.4f}\n".format(1000 * total_loss / len(test_data))
                log.write(log_entry)
                print(log_entry.strip())  # æ‰“å°åˆ°æ§åˆ¶å°

    # ä¸‰ç§æŒ‡æ ‡å–å¹³å‡
    print("Performance:  MAE {:2.2f}    {:2.2f}%    {:2.2f}".format(np.mean(MAE), np.mean(MAPE * 100), np.mean(RMSE)))

    # å°†ç¬¬0è¡Œçš„0åˆ é™¤ï¼Œå› ä¸ºå¼€å§‹å®šä¹‰çš„æ—¶å€™ç”¨0å¡«å……ï¼Œä½†æ˜¯æ—¶é—´æ˜¯ä»1å¼€å§‹çš„
    Predict = np.delete(Predict, 0, axis=1)
    Target = np.delete(Target, 0, axis=1)

    result_file = "GAT_08_result.h5" ##todo
    file_obj = h5py.File(result_file, "w")  # å°†é¢„æµ‹å€¼å’Œç›®æ ‡å€¼ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œå› ä¸ºè¦å¤šæ¬¡å¯è§†åŒ–çœ‹çœ‹ç»“æœ

    file_obj["predict"] = Predict  # [N, T, D]
    file_obj["target"] = Target  # [N, T, D]

def extract_loss_from_log(log_file):
    """
    ä»æ—¥å¿—æ–‡ä»¶ä¸­æå–è®­ç»ƒå’Œæµ‹è¯•çš„æŸå¤±å€¼
    """
    train_losses = []
    test_losses = []
    with open(log_file, "r") as f:
        for line in f:
            # æå–è®­ç»ƒæŸå¤±
            train_match = re.search(r"Epoch: (\d+), Loss: (\d+\.\d+)", line)
            if train_match:
                epoch = int(train_match.group(1))
                loss = float(train_match.group(2))
                train_losses.append((epoch, loss))

            # æå–æµ‹è¯•æŸå¤±
            test_match = re.search(r"Test Loss: (\d+\.\d+)", line)
            if test_match:
                loss = float(test_match.group(1))
                test_losses.append(loss)

    return train_losses, test_losses

def compute_performance(prediction, target, data):  # è®¡ç®—æ¨¡å‹æ€§èƒ½
    try:
        dataset = data.dataset  # æ•°æ®ä¸ºdataloaderå‹ï¼Œé€šè¿‡å®ƒä¸‹é¢çš„å±æ€§.datasetç±»å˜æˆdatasetå‹æ•°æ®
    except:
        dataset = data  # æ•°æ®ä¸ºdatasetå‹ï¼Œç›´æ¥èµ‹å€¼

    # ä¸‹é¢å°±æ˜¯å¯¹é¢„æµ‹å’Œç›®æ ‡æ•°æ®è¿›è¡Œé€†å½’ä¸€åŒ–
    #  flow_normä¸ºå½’ä¸€åŒ–çš„åŸºï¼Œflow_norm[0]ä¸ºæœ€å¤§å€¼ï¼Œflow_norm[1]ä¸ºæœ€å°å€¼
    # prediction.numpy()å’Œtarget.numpy()æ˜¯éœ€è¦é€†å½’ä¸€åŒ–çš„æ•°æ®ï¼Œè½¬æ¢æˆnumpyå‹æ˜¯å› ä¸º recover_data()å‡½æ•°ä¸­çš„æ•°æ®éƒ½æ˜¯numpyå‹ï¼Œä¿æŒä¸€è‡´
    prediction = LoadData.recover_data(
        dataset.flow_norm[0], dataset.flow_norm[1], prediction.numpy())
    target = LoadData.recover_data(
        dataset.flow_norm[0], dataset.flow_norm[1], target.numpy())

    # å¯¹ä¸‰ç§è¯„ä»·æŒ‡æ ‡å†™äº†ä¸€ä¸ªç±»ï¼Œè¿™ä¸ªç±»å°è£…åœ¨å¦ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œåœ¨åé¢
    mae, mape, rmse = Evaluation.total(
        target.reshape(-1), prediction.reshape(-1))  # å˜æˆå¸¸å‘é‡æ‰èƒ½è®¡ç®—è¿™ä¸‰ç§æŒ‡æ ‡

    performance = [mae, mape, rmse]
    recovered_data = [prediction, target]

    return performance, recovered_data  # è¿”å›è¯„ä»·ç»“æœï¼Œä»¥åŠæ¢å¤å¥½çš„æ•°æ®ï¼ˆä¸ºå¯è§†åŒ–å‡†å¤‡çš„ï¼‰


if __name__ == '__main__':
    main()

    # æå–æŸå¤±å€¼
    log_file = "GAT_08_training_log.txt" ##todo
    train_losses, test_losses = extract_loss_from_log(log_file)

    # æ‰“å°æå–çš„æŸå¤±å€¼
    print("Train Losses:", train_losses)
    print("Test Losses:", test_losses)

    visualize_result(h5_file="GAT_08_result.h5", ##todo
                     nodes_id=120, time_se=[0, 24 * 12 * 2],  # æ˜¯èŠ‚ç‚¹çš„æ—¶é—´èŒƒå›´
                     visualize_file="gat_node_120")
