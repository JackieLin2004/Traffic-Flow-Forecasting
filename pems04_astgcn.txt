root@autodl-container-4d0049839c-4cc11c6f:~/autodl-tmp/Traffic-Flow-Forecasting/ASTGCN# python prepareData.py
Read configuration file: configurations/PEMS04_astgcn.conf
mean.shape: (1, 1, 3, 1)
std.shape: (1, 1, 3, 1)
train x: (10181, 307, 3, 12)
train target: (10181, 307, 12)
train timestamp: (10181, 1)

val x: (3394, 307, 3, 12)
val target: (3394, 307, 12)
val timestamp: (3394, 1)

test x: (3394, 307, 3, 12)
test target: (3394, 307, 12)
test timestamp: (3394, 1)

train data _mean : (1, 1, 3, 1) [[[[2.07227338e+02]
   [5.13195612e-02]
   [6.34740574e+01]]]]
train data _std : (1, 1, 3, 1) [[[[1.56477655e+02]
   [4.78541626e-02]
   [8.10351724e+00]]]]
save file: ./data/PEMS04/PEMS04_r1_d0_w0_astcgn



root@autodl-container-4d0049839c-4cc11c6f:~/autodl-tmp/Traffic-Flow-Forecasting/ASTGCN# python train_ASTGCN_r.py 
Read configuration file: ./configurations/PEMS04_astgcn.conf
CUDA: True cuda:0
folder_dir: astgcn_r_h1d0w0_channel1_1.000000e-03
params_path: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03
load file: ./data/PEMS04/PEMS04_r1_d0_w0_astcgn
train: torch.Size([10181, 307, 1, 12]) torch.Size([10181, 307, 12])
val: torch.Size([3394, 307, 1, 12]) torch.Size([3394, 307, 12])
test: torch.Size([3394, 307, 1, 12]) torch.Size([3394, 307, 12])
delete the old one and create params directory experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03
param list:
CUDA     cuda:0
in_channels      1
nb_block         2
nb_chev_filter   64
nb_time_filter   64
time_strides     1
batch_size       32
graph_signal_matrix_filename     ./data/PEMS04/PEMS04.npz
start_epoch      0
epochs   80
ASTGCN_submodule(
  (BlockList): ModuleList(
    (0): ASTGCN_block(
      (TAt): Temporal_Attention_layer()
      (SAt): Spatial_Attention_layer()
      (cheb_conv_SAt): cheb_conv_withSAt(
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
        )
      )
      (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (residual_conv): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 1))
      (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (1): ASTGCN_block(
      (TAt): Temporal_Attention_layer()
      (SAt): Spatial_Attention_layer()
      (cheb_conv_SAt): cheb_conv_withSAt(
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
        )
      )
      (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (residual_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (final_conv): Conv2d(12, 12, kernel_size=(1, 64), stride=(1, 1))
)
Net's state_dict:
BlockList.0.TAt.U1       torch.Size([307])
BlockList.0.TAt.U2       torch.Size([1, 307])
BlockList.0.TAt.U3       torch.Size([1])
BlockList.0.TAt.be       torch.Size([1, 12, 12])
BlockList.0.TAt.Ve       torch.Size([12, 12])
BlockList.0.SAt.W1       torch.Size([12])
BlockList.0.SAt.W2       torch.Size([1, 12])
BlockList.0.SAt.W3       torch.Size([1])
BlockList.0.SAt.bs       torch.Size([1, 307, 307])
BlockList.0.SAt.Vs       torch.Size([307, 307])
BlockList.0.cheb_conv_SAt.Theta.0        torch.Size([1, 64])
BlockList.0.cheb_conv_SAt.Theta.1        torch.Size([1, 64])
BlockList.0.cheb_conv_SAt.Theta.2        torch.Size([1, 64])
BlockList.0.time_conv.weight     torch.Size([64, 64, 1, 3])
BlockList.0.time_conv.bias       torch.Size([64])
BlockList.0.residual_conv.weight         torch.Size([64, 1, 1, 1])
BlockList.0.residual_conv.bias   torch.Size([64])
BlockList.0.ln.weight    torch.Size([64])
BlockList.0.ln.bias      torch.Size([64])
BlockList.1.TAt.U1       torch.Size([307])
BlockList.1.TAt.U2       torch.Size([64, 307])
BlockList.1.TAt.U3       torch.Size([64])
BlockList.1.TAt.be       torch.Size([1, 12, 12])
BlockList.1.TAt.Ve       torch.Size([12, 12])
BlockList.1.SAt.W1       torch.Size([12])
BlockList.1.SAt.W2       torch.Size([64, 12])
BlockList.1.SAt.W3       torch.Size([64])
BlockList.1.SAt.bs       torch.Size([1, 307, 307])
BlockList.1.SAt.Vs       torch.Size([307, 307])
BlockList.1.cheb_conv_SAt.Theta.0        torch.Size([64, 64])
BlockList.1.cheb_conv_SAt.Theta.1        torch.Size([64, 64])
BlockList.1.cheb_conv_SAt.Theta.2        torch.Size([64, 64])
BlockList.1.time_conv.weight     torch.Size([64, 64, 1, 3])
BlockList.1.time_conv.bias       torch.Size([64])
BlockList.1.residual_conv.weight         torch.Size([64, 64, 1, 1])
BlockList.1.residual_conv.bias   torch.Size([64])
BlockList.1.ln.weight    torch.Size([64])
BlockList.1.ln.bias      torch.Size([64])
final_conv.weight        torch.Size([12, 12, 1, 64])
final_conv.bias          torch.Size([12])
Net's total params: 450031
Optimizer's state_dict:
state    {}
param_groups     [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]}]
validation batch 1 / 107, loss: 275.83
validation batch 101 / 107, loss: 332.01
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_0.params
validation batch 1 / 107, loss: 55.45
validation batch 101 / 107, loss: 82.68
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_1.params
validation batch 1 / 107, loss: 37.24
validation batch 101 / 107, loss: 46.58
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_2.params
validation batch 1 / 107, loss: 33.13
validation batch 101 / 107, loss: 37.57
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_3.params
global step: 1000, training loss: 24.15, time: 86.49s
validation batch 1 / 107, loss: 30.69
validation batch 101 / 107, loss: 34.27
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_4.params
validation batch 1 / 107, loss: 29.10
validation batch 101 / 107, loss: 33.33
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_5.params
validation batch 1 / 107, loss: 28.12
validation batch 101 / 107, loss: 32.49
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_6.params
global step: 2000, training loss: 19.93, time: 171.47s
validation batch 1 / 107, loss: 27.82
validation batch 101 / 107, loss: 32.45
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_7.params
validation batch 1 / 107, loss: 28.87
validation batch 101 / 107, loss: 32.91
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_8.params
validation batch 1 / 107, loss: 27.41
validation batch 101 / 107, loss: 31.26
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_9.params
global step: 3000, training loss: 20.48, time: 253.11s
validation batch 1 / 107, loss: 27.26
validation batch 101 / 107, loss: 31.05
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_10.params
validation batch 1 / 107, loss: 27.68
validation batch 101 / 107, loss: 31.02
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_11.params
validation batch 1 / 107, loss: 26.85
validation batch 101 / 107, loss: 30.44
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_12.params
global step: 4000, training loss: 21.21, time: 412.69s
validation batch 1 / 107, loss: 27.21
validation batch 101 / 107, loss: 30.95
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_13.params
validation batch 1 / 107, loss: 26.90
validation batch 101 / 107, loss: 30.34
validation batch 1 / 107, loss: 27.78
validation batch 101 / 107, loss: 30.91
global step: 5000, training loss: 21.30, time: 610.36s
validation batch 1 / 107, loss: 27.45
validation batch 101 / 107, loss: 30.24
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_16.params
validation batch 1 / 107, loss: 27.13
validation batch 101 / 107, loss: 29.95
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_17.params
validation batch 1 / 107, loss: 26.40
validation batch 101 / 107, loss: 29.54
global step: 6000, training loss: 18.85, time: 800.01s
validation batch 1 / 107, loss: 26.61
validation batch 101 / 107, loss: 29.99
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_19.params
validation batch 1 / 107, loss: 26.21
validation batch 101 / 107, loss: 29.74
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_20.params
validation batch 1 / 107, loss: 26.32
validation batch 101 / 107, loss: 29.70
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_21.params
global step: 7000, training loss: 19.98, time: 1026.61s
validation batch 1 / 107, loss: 26.29
validation batch 101 / 107, loss: 29.37
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_22.params
validation batch 1 / 107, loss: 27.38
validation batch 101 / 107, loss: 30.39
validation batch 1 / 107, loss: 26.14
validation batch 101 / 107, loss: 29.31
validation batch 1 / 107, loss: 26.27
validation batch 101 / 107, loss: 29.95
global step: 8000, training loss: 19.48, time: 1203.67s
validation batch 1 / 107, loss: 25.82
validation batch 101 / 107, loss: 29.02
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_26.params
validation batch 1 / 107, loss: 26.19
validation batch 101 / 107, loss: 29.40
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_27.params
validation batch 1 / 107, loss: 26.48
validation batch 101 / 107, loss: 29.72
global step: 9000, training loss: 19.78, time: 1374.84s
validation batch 1 / 107, loss: 25.94
validation batch 101 / 107, loss: 29.58
validation batch 1 / 107, loss: 25.91
validation batch 101 / 107, loss: 29.56
validation batch 1 / 107, loss: 26.04
validation batch 101 / 107, loss: 29.39
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_31.params
global step: 10000, training loss: 18.24, time: 1542.73s
validation batch 1 / 107, loss: 26.59
validation batch 101 / 107, loss: 29.31
validation batch 1 / 107, loss: 25.96
validation batch 101 / 107, loss: 29.40
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_33.params
validation batch 1 / 107, loss: 26.80
validation batch 101 / 107, loss: 29.47
global step: 11000, training loss: 20.18, time: 1716.03s
validation batch 1 / 107, loss: 25.99
validation batch 101 / 107, loss: 29.50
validation batch 1 / 107, loss: 26.74
validation batch 101 / 107, loss: 29.52
validation batch 1 / 107, loss: 25.94
validation batch 101 / 107, loss: 29.46
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_37.params
global step: 12000, training loss: 19.24, time: 1887.69s
validation batch 1 / 107, loss: 25.91
validation batch 101 / 107, loss: 29.21
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_38.params
validation batch 1 / 107, loss: 25.74
validation batch 101 / 107, loss: 29.73
validation batch 1 / 107, loss: 26.05
validation batch 101 / 107, loss: 29.39
global step: 13000, training loss: 16.63, time: 2063.49s
validation batch 1 / 107, loss: 25.90
validation batch 101 / 107, loss: 29.38
validation batch 1 / 107, loss: 26.09
validation batch 101 / 107, loss: 29.64
validation batch 1 / 107, loss: 25.84
validation batch 101 / 107, loss: 30.07
global step: 14000, training loss: 20.62, time: 2234.54s
validation batch 1 / 107, loss: 26.28
validation batch 101 / 107, loss: 29.78
validation batch 1 / 107, loss: 25.90
validation batch 101 / 107, loss: 29.49
validation batch 1 / 107, loss: 26.04
validation batch 101 / 107, loss: 29.53
validation batch 1 / 107, loss: 25.81
validation batch 101 / 107, loss: 30.37
global step: 15000, training loss: 16.90, time: 2473.75s
validation batch 1 / 107, loss: 25.77
validation batch 101 / 107, loss: 29.63
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_48.params
validation batch 1 / 107, loss: 26.11
validation batch 101 / 107, loss: 29.41
validation batch 1 / 107, loss: 25.95
validation batch 101 / 107, loss: 29.74
global step: 16000, training loss: 20.22, time: 2727.39s
validation batch 1 / 107, loss: 25.70
validation batch 101 / 107, loss: 29.72
save parameters to file: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_51.params
validation batch 1 / 107, loss: 25.82
validation batch 101 / 107, loss: 29.95
validation batch 1 / 107, loss: 25.72
validation batch 101 / 107, loss: 30.30
global step: 17000, training loss: 16.80, time: 2951.04s
validation batch 1 / 107, loss: 25.88
validation batch 101 / 107, loss: 29.85
validation batch 1 / 107, loss: 25.94
validation batch 101 / 107, loss: 29.61
validation batch 1 / 107, loss: 25.81
validation batch 101 / 107, loss: 29.84
global step: 18000, training loss: 18.26, time: 3123.90s
validation batch 1 / 107, loss: 26.06
validation batch 101 / 107, loss: 29.99
validation batch 1 / 107, loss: 25.88
validation batch 101 / 107, loss: 30.13
validation batch 1 / 107, loss: 25.86
validation batch 101 / 107, loss: 30.13
global step: 19000, training loss: 16.79, time: 3336.02s
validation batch 1 / 107, loss: 26.34
validation batch 101 / 107, loss: 29.79
validation batch 1 / 107, loss: 25.72
validation batch 101 / 107, loss: 30.12
validation batch 1 / 107, loss: 25.95
validation batch 101 / 107, loss: 30.40
global step: 20000, training loss: 19.15, time: 3506.16s
validation batch 1 / 107, loss: 25.91
validation batch 101 / 107, loss: 30.16
validation batch 1 / 107, loss: 25.80
validation batch 101 / 107, loss: 30.00
validation batch 1 / 107, loss: 25.74
validation batch 101 / 107, loss: 30.21
global step: 21000, training loss: 16.74, time: 3673.76s
validation batch 1 / 107, loss: 25.66
validation batch 101 / 107, loss: 29.87
validation batch 1 / 107, loss: 25.86
validation batch 101 / 107, loss: 29.92
validation batch 1 / 107, loss: 25.89
validation batch 101 / 107, loss: 30.41
global step: 22000, training loss: 17.99, time: 3838.48s
validation batch 1 / 107, loss: 25.91
validation batch 101 / 107, loss: 30.04
validation batch 1 / 107, loss: 25.81
validation batch 101 / 107, loss: 29.87
validation batch 1 / 107, loss: 25.80
validation batch 101 / 107, loss: 30.10
validation batch 1 / 107, loss: 25.78
validation batch 101 / 107, loss: 30.47
global step: 23000, training loss: 19.90, time: 3930.83s
validation batch 1 / 107, loss: 25.99
validation batch 101 / 107, loss: 30.48
validation batch 1 / 107, loss: 26.03
validation batch 101 / 107, loss: 30.51
validation batch 1 / 107, loss: 25.74
validation batch 101 / 107, loss: 30.33
global step: 24000, training loss: 16.12, time: 4022.00s
validation batch 1 / 107, loss: 25.76
validation batch 101 / 107, loss: 30.19
validation batch 1 / 107, loss: 25.93
validation batch 101 / 107, loss: 29.81
validation batch 1 / 107, loss: 25.94
validation batch 101 / 107, loss: 30.47
global step: 25000, training loss: 18.34, time: 4114.10s
validation batch 1 / 107, loss: 25.97
validation batch 101 / 107, loss: 29.88
best epoch: 51
load weight from: experiments/PEMS04/astgcn_r_h1d0w0_channel1_1.000000e-03/epoch_51.params
predicting data set batch 1 / 107
predicting data set batch 101 / 107
input: (3394, 307, 1, 12)
prediction: (3394, 307, 12)
data_target_tensor: (3394, 307, 12)
current epoch: 51, predict 0 points
MAE: 17.67
RMSE: 28.32
MAPE: 0.12
current epoch: 51, predict 1 points
MAE: 18.56
RMSE: 29.66
MAPE: 0.13
current epoch: 51, predict 2 points
MAE: 19.23
RMSE: 30.64
MAPE: 0.13
current epoch: 51, predict 3 points
MAE: 19.71
RMSE: 31.35
MAPE: 0.13
current epoch: 51, predict 4 points
MAE: 20.11
RMSE: 31.95
MAPE: 0.13
current epoch: 51, predict 5 points
MAE: 20.49
RMSE: 32.51
MAPE: 0.14
current epoch: 51, predict 6 points
MAE: 20.93
RMSE: 33.14
MAPE: 0.14
current epoch: 51, predict 7 points
MAE: 21.37
RMSE: 33.76
MAPE: 0.14
current epoch: 51, predict 8 points
MAE: 21.85
RMSE: 34.41
MAPE: 0.15
current epoch: 51, predict 9 points
MAE: 22.40
RMSE: 35.16
MAPE: 0.15
current epoch: 51, predict 10 points
MAE: 23.06
RMSE: 36.04
MAPE: 0.15
current epoch: 51, predict 11 points
MAE: 23.87
RMSE: 37.12
MAPE: 0.16
all MAE: 20.77
all RMSE: 32.93
all MAPE: 0.14
[17.672804, 28.321925600611312, 0.11871776, 18.55575, 29.65649902753716, 0.12524344, 19.232847, 30.643159674930413, 0.13032661, 19.709707, 31.35390538373968, 0.13311946, 20.105253, 31.948739002159773, 0.13481547, 20.494942, 32.51108209494687, 0.13756208, 20.932373, 33.135913591317355, 0.13928685, 21.367374, 33.75600170883968, 0.14243338, 21.84544, 34.41344263772062, 0.14553909, 22.399204, 35.155277764334336, 0.14895459, 23.05999, 36.0393495612348, 0.15206137, 23.872965, 37.12221817229205, 0.15645689, 20.770723, 32.93420509492183, 0.13871008]